from __future__ import annotations
import os, json, math, random
from dataclasses import dataclass
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from pathlib import Path
from tensorflow.keras.callbacks import ModelCheckpoint

from townPred.pretrain_plot import plot_trends

# (ì„ íƒ) ë„¤ ëª¨ë“ˆì´ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë°ëª¨ ë°ì´í„°ë¡œ ëŒ€ì²´
try:
    from townPred.data_prex import data_prex
except Exception:
    data_prex = None

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# =========================
# ì„¤ì •ê°’
# =========================
SEED = 42
N_SPLITS = 5
SEQ_LEN = 5
HORIZON = 1
BATCH_SIZE = 512
EPOCHS = 100
PATIENCE = 10
UNITS = 64
DROPOUT = 0.1
LEARNING_RATE = 1e-3
SHOCK_ENABLE   = False
SHOCK_START    = "2021-03-01"
SHOCK_END      = "2022-09-01"
SHOCK_AMP_FROM = -1000.0
SHOCK_AMP_TO   =  1000.0
SHOCK_TARGET_LOCS: List[str] | None = None   #ì§€ì—­ì½”ë“œ (shock)


def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
set_seed(SEED)

def rmse(y_true, y_pred):
    return math.sqrt(mean_squared_error(y_true, y_pred))

def mape(y_true, y_pred, eps: float = 1e-8):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    return np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100.0

def prepare_df(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out["date"] = pd.to_datetime(out["date"])
    out = out.sort_values(["location_code","date"]).reset_index(drop=True)
    return out

def make_time_folds(df: pd.DataFrame, n_splits: int) -> List[Tuple[pd.Timestamp, pd.Timestamp]]:
    uniq_dates = np.array(sorted(df["date"].unique()))
    fold_boundaries = np.linspace(SEQ_LEN + HORIZON, len(uniq_dates) - 1, n_splits + 1, dtype=int)
    folds = []
    for i in range(n_splits):
        test_start_idx = fold_boundaries[i]
        test_end_idx   = fold_boundaries[i + 1]
        test_start = uniq_dates[test_start_idx]
        test_end   = uniq_dates[test_end_idx]
        folds.append((test_start, test_end))
    return folds

def inject_shock(df: pd.DataFrame,
                 start: str,
                 end: str,
                 amp_start: float,
                 amp_end: float,
                 target_locs: List[str] | None = None) -> pd.DataFrame: #ì§€ì—­ì½”ë“œ locs
    out = df.copy()
    out["date"] = pd.to_datetime(out["date"])
    m = (out["date"] >= pd.Timestamp(start)) & (out["date"] <= pd.Timestamp(end))
    if target_locs is not None:
        m &= out["location_code"].isin(target_locs)

    # í•´ë‹¹ êµ¬ê°„ì˜ ê³ ìœ  ë‚ ì§œë“¤ì— ëŒ€í•´ ì„ í˜• shock ê³¡ì„  ìƒì„±
    dates = out.loc[m, "date"].sort_values().unique()
    if len(dates) == 0:
        # ì ìš© êµ¬ê°„ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return out

    shock_curve = np.linspace(amp_start, amp_end, len(dates))
    shock_map = {d: v for d, v in zip(dates, shock_curve)}

    # valueì— shock ë”í•˜ê¸°
    out.loc[m, "value"] = (
            out.loc[m, "value"].astype(float)
            + out.loc[m, "date"].map(shock_map).astype(float)
    )
    out["value"] = out["value"].clip(lower=0)  # ìŒìˆ˜ ë°©ì§€
    return out

def build_windows_for_range(
        df, train_end_date, test_start_date, test_end_date,
        seq_len, horizon, fit_scaler, scalers=None
):
    if scalers is None:
        scalers = {}

    X_train_list, y_train_list = [], []
    X_test_list,  y_test_list  = [], []
    meta_te = []

    for loc, g in df.groupby("location_code"):
        g = g.sort_values("date").reset_index(drop=True)

        g_train = g[g["date"] < test_start_date] if train_end_date is None else g[g["date"] <= train_end_date]
        g_test  = g[(g["date"] >= test_start_date) & (g["date"] <= test_end_date)]

        # âœ… train ìƒ˜í”Œ ì—†ìœ¼ë©´ ì´ ì§€ì—­ì€ ì™„ì „íˆ ìŠ¤í‚µ(ìŠ¤ì¼€ì¼ëŸ¬ë„ ë§Œë“¤ì§€ ì•ŠìŒ)
        if len(g_train) == 0:
            continue

        # âœ… ì´ì œ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë§Œë“  ë’¤ fit
        if loc not in scalers:
            scalers[loc] = StandardScaler()
        if fit_scaler:
            train_vals_all = g_train["value"].to_numpy(dtype=float).reshape(-1, 1)
            scalers[loc].fit(train_vals_all)

        # ----- ìœˆë„ìš° ë§Œë“¤ê¸° ë³´ì¡° -----
        def make_windows(arr_values, arr_dates, collect_meta: bool):
            Xs, ys, metas = [], [], []
            for t in range(len(arr_values) - seq_len - horizon + 1):
                Xs.append(arr_values[t:t+seq_len])
                ys.append(arr_values[t+seq_len + horizon - 1])
                if collect_meta:
                    metas.append((loc, pd.to_datetime(arr_dates[t+seq_len + horizon - 1])))
            return np.array(Xs), np.array(ys), metas

        # ---- Train ----
        if len(g_train) >= seq_len + horizon:
            train_vals = g_train["value"].to_numpy(dtype=float).reshape(-1, 1)
            train_sc = scalers[loc].transform(train_vals).squeeze(-1)
            Xtr, ytr, _ = make_windows(train_sc, g_train["date"].values, collect_meta=False)
            if len(Xtr):
                X_train_list.append(Xtr[..., np.newaxis])
                y_train_list.append(ytr.reshape(-1, 1))

        # ---- Test ----
        if len(g_test) >= seq_len + horizon:
            test_vals = g_test["value"].to_numpy(dtype=float).reshape(-1, 1)
            test_sc = scalers[loc].transform(test_vals).squeeze(-1)
            Xte, yte, metas = make_windows(test_sc, g_test["date"].values, collect_meta=True)
            if len(Xte):
                X_test_list.append(Xte[..., np.newaxis])
                y_test_list.append(yte.reshape(-1, 1))
                meta_te.extend(metas)

    def cat_or_empty(lst, shape):
        if lst:
            return np.concatenate(lst, axis=0)
        return np.zeros(shape, dtype=float)

    X_train = cat_or_empty(X_train_list, (0, seq_len, 1))
    y_train = np.concatenate(y_train_list, axis=0) if y_train_list else np.zeros((0, 1), dtype=float)
    X_test  = cat_or_empty(X_test_list,  (0, seq_len, 1))
    y_test  = np.concatenate(y_test_list,  axis=0) if y_test_list  else np.zeros((0, 1), dtype=float)

    return X_train, y_train, X_test, y_test, scalers, meta_te



def build_lstm(input_shape, units=64, dropout=0.1, bidirectional=False):
    inp = keras.Input(shape=input_shape)
    if bidirectional:
        x = keras.layers.Bidirectional(keras.layers.LSTM(units, return_sequences=False))(inp)
    else:
        x = keras.layers.LSTM(units, return_sequences=False)(inp)
    if dropout > 0:
        x = keras.layers.Dropout(dropout)(x)
    out = keras.layers.Dense(1)(x)
    model = keras.Model(inp, out)
    opt = keras.optimizers.Adam(learning_rate=LEARNING_RATE)
    model.compile(optimizer=opt, loss="mse")
    return model


# =========================
# ë©”ì¸: êµì°¨ê²€ì¦ ë£¨í”„
# =========================
@dataclass
class FoldResult:
    model_type: str
    fold_idx: int
    n_train: int
    n_test: int
    rmse: float
    mae: float
    mape: float


def run_cv_compare(df: pd.DataFrame, loc_focus: str | None = None, return_predictions: bool = True):
    """
    ë°˜í™˜ (return_predictions=Trueì¼ ë•Œ):
      df_pred: columns = ["date", "y_true_sc", "y_pred_uni_sc", "y_pred_bi_sc"]
    -> plot_trends(df_pred)ë¡œ ë°”ë¡œ ë„ì‹í™” ê°€ëŠ¥
    (ì£¼ì˜: ì´ ë²„ì „ì€ 'ìŠ¤ì¼€ì¼ ë‹¨ìœ„' ì§€í‘œ ê³„ì‚°. ì›ë‹¨ìœ„ ì§€í‘œê°€ í•„ìš”í•˜ë©´ ì•Œë ¤ì¤˜!)
    """
    df = prepare_df(df)
    folds = make_time_folds(df, N_SPLITS)

    all_results: List[FoldResult] = []
    pred_rows: List[Tuple[pd.Timestamp, float, float, float]] = []  # (date, y_true_sc, y_pred_uni_sc, y_pred_bi_sc)

    # ê³µí†µ ì½œë°±
    def make_callbacks(tag: str):
        os.makedirs("./checkpoints", exist_ok=True)
        return [
            ModelCheckpoint(
                filepath=f"./checkpoints/pretrained_{tag}.weights.h5",
                save_best_only=True,
                save_weights_only=True,
                monitor="val_loss",
                mode="min",
                verbose=1
            ),
            keras.callbacks.EarlyStopping(
                monitor="val_loss", patience=PATIENCE, restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor="val_loss", factor=0.5, patience=max(2, PATIENCE // 2), verbose=0
            ),
        ]

    for fi, (test_start, test_end) in enumerate(folds, start=1):
        # Train êµ¬ê°„ì€ test_start ì´ì „ ì „ì²´(í™•ì¥í˜•)
        train_end = test_start - pd.offsets.MonthBegin(1)  # ì§ì „ ë‹¬ ë§
        # ê³µìš© ìŠ¤ì¼€ì¼ëŸ¬ ë”•ì…”ë„ˆë¦¬(ì§€ì—­ë³„) â€” ê°™ì€ í´ë“œ ë‚´ ëª¨ë¸ ê°„ ê³µìœ í•´ì„œ ê³µì • ë¹„êµ
        scalers: Dict[str, StandardScaler] = {}

        # ë°ì´í„° ë‚˜ëˆ„ê¸° (fit_scaler=Trueë¡œ í•œ ë²ˆë§Œ)
        Xtr, ytr, Xte, yte, scalers, meta_te = build_windows_for_range(
            df, train_end, test_start, test_end,
            seq_len=SEQ_LEN, horizon=HORIZON,
            fit_scaler=True, scalers=scalers
        )
        if len(Xtr) == 0 or len(Xte) == 0:
            print(f"[Fold {fi}] ìƒ˜í”Œì´ ë¶€ì¡±í•˜ì—¬ ê±´ë„ˆëœ€ (train {Xtr.shape}, test {Xte.shape})")
            continue

        input_shape = (SEQ_LEN, 1)

        # ---- ë‹¨ë°©í–¥ LSTM ----
        model_uni = build_lstm(input_shape, units=UNITS, dropout=DROPOUT, bidirectional=False)
        model_uni.fit(
            Xtr, ytr, validation_split=0.1,
            epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,
            callbacks=make_callbacks("uni")
        )
        y_pred_uni_sc = model_uni.predict(Xte, batch_size=BATCH_SIZE, verbose=0).squeeze(-1)

        # ---- ì–‘ë°©í–¥ LSTM ----
        model_bi = build_lstm(input_shape, units=UNITS, dropout=DROPOUT, bidirectional=True)
        model_bi.fit(
            Xtr, ytr, validation_split=0.1,
            epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,
            callbacks=make_callbacks("bi")
        )
        y_pred_bi_sc = model_bi.predict(Xte, batch_size=BATCH_SIZE, verbose=0).squeeze(-1)

        # í‰ê°€(ìŠ¤ì¼€ì¼ ë‹¨ìœ„)
        y_true_sc = yte.squeeze(-1)
        res_uni = FoldResult(
            "LSTM-Uni", fi, len(Xtr), len(Xte),
            rmse=rmse(y_true_sc, y_pred_uni_sc),
            mae=mean_absolute_error(y_true_sc, y_pred_uni_sc),
            mape=mape(y_true_sc, y_pred_uni_sc)
        )
        res_bi = FoldResult(
            "LSTM-Bi", fi, len(Xtr), len(Xte),
            rmse=rmse(y_true_sc, y_pred_bi_sc),
            mae=mean_absolute_error(y_true_sc, y_pred_bi_sc),
            mape=mape(y_true_sc, y_pred_bi_sc)
        )
        all_results.extend([res_uni, res_bi])

        print(f"[Fold {fi}] Uni  RMSE={res_uni.rmse:.4f} MAE={res_uni.mae:.4f} MAPE={res_uni.mape:.2f}%")
        print(f"[Fold {fi}] Bi   RMSE={res_bi.rmse:.4f} MAE={res_bi.mae:.4f} MAPE={res_bi.mape:.2f}%")

        # === ë„ì‹í™”ë¥¼ ìœ„í•œ ì˜ˆì¸¡ ìˆ˜ì§‘ ===
        for (loc, d), yt, pu, pb in zip(meta_te, y_true_sc, y_pred_uni_sc, y_pred_bi_sc):
            if (loc_focus is None) or (loc == loc_focus):
                pred_rows.append((pd.to_datetime(d), float(yt), float(pu), float(pb)))

    # ìš”ì•½ í…Œì´ë¸”
    if not all_results:
        print("ìœ íš¨í•œ í´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        if return_predictions:
            return pd.DataFrame(columns=["date", "y_true_sc", "y_pred_uni_sc", "y_pred_bi_sc"])
        return

    summary = (
        pd.DataFrame([r.__dict__ for r in all_results])
        .groupby("model_type")[["rmse","mae","mape"]]
        .mean()
        .sort_values("rmse")
    )
    print("\n=== êµì°¨ê²€ì¦ í‰ê·  ì„±ëŠ¥(ìŠ¤ì¼€ì¼ ë‹¨ìœ„) ===")
    print(summary)

    best = summary.index[0]
    print(f"\nğŸ‘‰ í‰ê·  RMSE ê¸°ì¤€ ìµœê³  ëª¨ë¸: {best}")

    # ìµœì¢… ì˜ˆì¸¡ DF ìƒì„± & ë¦¬í„´
    if return_predictions:
        df_pred = pd.DataFrame(
            pred_rows,
            columns=["date","y_true_sc","y_pred_uni_sc","y_pred_bi_sc"]
        ).sort_values("date").reset_index(drop=True)
        return df_pred



if data_prex is not None:
    try:
        df = data_prex()  # columns: date, location_code, value
    except Exception:
        df = None
else:
    df = None
if SHOCK_ENABLE:
    df = inject_shock(
        df,
        start=SHOCK_START,
        end=SHOCK_END,
        amp_start=SHOCK_AMP_FROM,
        amp_end=SHOCK_AMP_TO,
        target_locs=SHOCK_TARGET_LOCS
    )
    print(f"[INFO] Shock injected: {SHOCK_START} ~ {SHOCK_END}, "
          f"{SHOCK_AMP_FROM} â†’ {SHOCK_AMP_TO}, "
          f"targets={SHOCK_TARGET_LOCS if SHOCK_TARGET_LOCS else 'ALL'}")

df_pred = run_cv_compare(df, loc_focus="1114060500", return_predictions=True)
plot_trends(df_pred)

