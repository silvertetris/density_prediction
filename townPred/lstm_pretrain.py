from __future__ import annotations
import os, json, math, random
from dataclasses import dataclass
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from pathlib import Path

from townPred.pretrain_plot import plot_trends

# (ì„ íƒ) ë„¤ ëª¨ë“ˆì´ ìˆë‹¤ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë°ëª¨ ë°ì´í„°ë¡œ ëŒ€ì²´
try:
    from townPred.data_prex import data_prex
except Exception:
    data_prex = None

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# =========================
# ì„¤ì •ê°’
# =========================
SEED            = 42
N_SPLITS        = 5                   # ì‹œê³„ì—´ í´ë“œ ìˆ˜ (walk-forward)
SEQ_LEN         = 5                   # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê¸¸ì´(ê°œì›”)
HORIZON         = 1                   # ì˜ˆì¸¡ ì‹œì (ë‹¤ìŒ ë‹¬)
BATCH_SIZE      = 512
EPOCHS          = 100
PATIENCE        = 10
UNITS           = 64
DROPOUT         = 0.1
LEARNING_RATE   = 1e-3

# =========================
# ì‹œë“œ ê³ ì •
# =========================
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

set_seed(SEED)

# =========================
# ìœ í‹¸: ì§€í‘œ
# =========================
def rmse(y_true, y_pred):
    return math.sqrt(mean_squared_error(y_true, y_pred))

def mape(y_true, y_pred, eps: float = 1e-8):
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    return np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100.0

def prepare_df(df: pd.DataFrame) -> pd.DataFrame:
    out = df.copy()
    out["date"] = pd.to_datetime(out["date"])
    out = out.sort_values(["location_code","date"]).reset_index(drop=True)
    return out

# í´ë“œ ìƒì„± (walk-forward: ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ êµ´ë¦¬ê¸°)
def make_time_folds(df: pd.DataFrame, n_splits: int) -> List[Tuple[pd.Timestamp, pd.Timestamp]]:
    # ìœ ë‹ˆí¬í•œ ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ í´ë“œ ìƒì„±
    uniq_dates = np.array(sorted(df["date"].unique()))
    # í…ŒìŠ¤íŠ¸ ë¸”ë¡ ê²½ê³„ ê³„ì‚° (trainì— ìµœì†Œ SEQ_LEN+HORIZON í™•ë³´)
    fold_boundaries = np.linspace(SEQ_LEN + HORIZON, len(uniq_dates)-1, n_splits+1, dtype=int)
    folds = []
    for i in range(n_splits):
        test_start_idx = fold_boundaries[i]
        test_end_idx   = fold_boundaries[i+1]
        test_start = uniq_dates[test_start_idx]
        test_end   = uniq_dates[test_end_idx]
        folds.append((test_start, test_end))
    return folds

# ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„± (ì—¬ëŸ¬ ì§€ì—­ì„ í•©ì³ í•˜ë‚˜ì˜ í•™ìŠµ ì„¸íŠ¸ë¡œ)
def build_windows_for_range(
    df: pd.DataFrame,
    train_end_date: pd.Timestamp | None,
    test_start_date: pd.Timestamp,
    test_end_date: pd.Timestamp,
    seq_len: int,
    horizon: int,
    fit_scaler: bool,
    scalers: Dict[str, StandardScaler] | None = None
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, StandardScaler], List[Tuple[str, pd.Timestamp]]]:

    X_train_list, y_train_list = [], []
    X_test_list,  y_test_list  = [], []
    meta_te: List[Tuple[str, pd.Timestamp]] = []

    if scalers is None:
        scalers = {}

    for loc, g in df.groupby("location_code"):
        g = g.sort_values("date").reset_index(drop=True)

        # split
        g_train = g[g["date"] < test_start_date] if train_end_date is None else g[g["date"] <= train_end_date]
        g_test  = g[(g["date"] >= test_start_date) & (g["date"] <= test_end_date)]

        # ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±
        if loc not in scalers:
            scalers[loc] = StandardScaler()

        # âš ï¸ ì—¬ê¸°ì„œ 'ìœˆë„ìš° ê°€ëŠ¥ ì—¬ë¶€'ì™€ ë³„ê°œë¡œ, train ìƒ˜í”Œë§Œìœ¼ë¡œë¼ë„ scalerëŠ” fit í•´ ë‘”ë‹¤
        if len(g_train) > 0:
            train_vals_all = g_train["value"].to_numpy(dtype=float).reshape(-1,1)
            if fit_scaler:
                scalers[loc].fit(train_vals_all)
        else:
            # ì´ ì§€ì—­ì€ train ë°ì´í„°ê°€ ì•„ì˜ˆ ì—†ìŒ â†’ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ fití•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ foldì—ì„œ ìŠ¤í‚µ
            continue

        # ìœˆë„ìš° ìƒì„± ë³´ì¡°
        def make_windows(arr_values: np.ndarray, arr_dates: np.ndarray, collect_meta: bool):
            Xs, ys, metas = [], [], []
            for t in range(len(arr_values) - seq_len - horizon + 1):
                Xs.append(arr_values[t:t+seq_len])
                ys.append(arr_values[t+seq_len + horizon - 1])
                if collect_meta:
                    metas.append((loc, pd.to_datetime(arr_dates[t+seq_len + horizon - 1])))
            return np.array(Xs), np.array(ys), metas

        # ---- Train ìœˆë„ìš° (ì¶©ë¶„í•  ë•Œë§Œ) ----
        if len(g_train) >= seq_len + horizon:
            train_vals = g_train["value"].to_numpy(dtype=float).reshape(-1,1)
            train_sc = scalers[loc].transform(train_vals).squeeze(-1)
            Xtr, ytr, _ = make_windows(train_sc, g_train["date"].values, collect_meta=False)
            if len(Xtr):
                X_train_list.append(Xtr[..., np.newaxis])
                y_train_list.append(ytr.reshape(-1,1))

        # ---- Test ìœˆë„ìš° (ì¶©ë¶„í•  ë•Œë§Œ) ----
        if len(g_test) >= seq_len + horizon:
            test_vals = g_test["value"].to_numpy(dtype=float).reshape(-1,1)
            # ì—¬ê¸°ì„œëŠ” ì´ë¯¸ ìœ„ì—ì„œ trainìœ¼ë¡œ scalerë¥¼ fit í–ˆê¸° ë•Œë¬¸ì— ì•ˆì „í•˜ê²Œ transform ê°€ëŠ¥
            test_sc = scalers[loc].transform(test_vals).squeeze(-1)
            Xte, yte, metas = make_windows(test_sc, g_test["date"].values, collect_meta=True)
            if len(Xte):
                X_test_list.append(Xte[..., np.newaxis])
                y_test_list.append(yte.reshape(-1,1))
                meta_te.extend(metas)

    def cat_or_empty(lst, shape):
        if lst:
            return np.concatenate(lst, axis=0)
        return np.zeros(shape, dtype=float)

    X_train = cat_or_empty(X_train_list, (0, seq_len, 1))
    y_train = np.concatenate(y_train_list, axis=0) if y_train_list else np.zeros((0,1), dtype=float)
    X_test  = cat_or_empty(X_test_list,  (0, seq_len, 1))
    y_test  = np.concatenate(y_test_list,  axis=0) if y_test_list  else np.zeros((0,1), dtype=float)

    return X_train, y_train, X_test, y_test, scalers, meta_te


# =========================
# ëª¨ë¸ ë¹Œë”
# =========================
def build_lstm(input_shape, units=64, dropout=0.1, bidirectional=False):
    inp = keras.Input(shape=input_shape)
    if bidirectional:
        x = keras.layers.Bidirectional(keras.layers.LSTM(units, return_sequences=False))(inp)
    else:
        x = keras.layers.LSTM(units, return_sequences=False)(inp)
    if dropout > 0:
        x = keras.layers.Dropout(dropout)(x)
    out = keras.layers.Dense(1)(x)
    model = keras.Model(inp, out)
    opt = keras.optimizers.Adam(learning_rate=LEARNING_RATE)
    model.compile(optimizer=opt, loss="mse")
    return model

# =========================
# ë©”ì¸: êµì°¨ê²€ì¦ ë£¨í”„
# =========================
@dataclass
class FoldResult:
    model_type: str
    fold_idx: int
    n_train: int
    n_test: int
    rmse: float
    mae: float
    mape: float

def run_cv_compare(df: pd.DataFrame, loc_focus: str | None = None, return_predictions: bool = True):
    """
    ë°˜í™˜ (return_predictions=Trueì¼ ë•Œ):
      df_pred: columns = ["date", "y_true_sc", "y_pred_uni_sc", "y_pred_bi_sc"]
    -> plot_trends(df_pred)ë¡œ ë°”ë¡œ ë„ì‹í™” ê°€ëŠ¥
    """
    df = prepare_df(df)
    folds = make_time_folds(df, N_SPLITS)

    all_results: List[FoldResult] = []
    pred_rows: List[Tuple[pd.Timestamp, float, float, float]] = []  # (date, y_true_sc, y_pred_uni_sc, y_pred_bi_sc)

    # ê³µí†µ ì½œë°±
    def make_callbacks(tag: str):
        return [
            keras.callbacks.EarlyStopping(
                monitor="val_loss", patience=PATIENCE, restore_best_weights=True
            ),
            keras.callbacks.ReduceLROnPlateau(
                monitor="val_loss", factor=0.5, patience=max(2, PATIENCE//2), verbose=0
            ),
        ]

    for fi, (test_start, test_end) in enumerate(folds, start=1):
        # Train êµ¬ê°„ì€ test_start ì´ì „ ì „ì²´(í™•ì¥í˜•)
        train_end = test_start - pd.offsets.MonthBegin(1)  # ì§ì „ ë‹¬ ë§
        # ê³µìš© ìŠ¤ì¼€ì¼ëŸ¬ ë”•ì…”ë„ˆë¦¬(ì§€ì—­ë³„) â€” ê°™ì€ í´ë“œ ë‚´ ëª¨ë¸ ê°„ ê³µìœ í•´ì„œ ê³µì • ë¹„êµ
        scalers: Dict[str, StandardScaler] = {}

        # ë°ì´í„° ë‚˜ëˆ„ê¸° (fit_scaler=Trueë¡œ í•œ ë²ˆë§Œ)
        Xtr, ytr, Xte, yte, scalers, meta_te = build_windows_for_range(
            df, train_end, test_start, test_end,
            seq_len=SEQ_LEN, horizon=HORIZON,
            fit_scaler=True, scalers=scalers
        )
        if len(Xtr) == 0 or len(Xte) == 0:
            print(f"[Fold {fi}] ìƒ˜í”Œì´ ë¶€ì¡±í•˜ì—¬ ê±´ë„ˆëœ€ (train {Xtr.shape}, test {Xte.shape})")
            continue

        input_shape = (SEQ_LEN, 1)

        # ---- ë‹¨ë°©í–¥ LSTM ----
        model_uni = build_lstm(input_shape, units=UNITS, dropout=DROPOUT, bidirectional=False)
        model_uni.fit(
            Xtr, ytr, validation_split=0.1,
            epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,
            callbacks=make_callbacks("uni")
        )
        y_pred_uni_sc = model_uni.predict(Xte, batch_size=BATCH_SIZE, verbose=0).squeeze(-1)

        # ---- ì–‘ë°©í–¥ LSTM ----
        model_bi = build_lstm(input_shape, units=UNITS, dropout=DROPOUT, bidirectional=True)
        model_bi.fit(
            Xtr, ytr, validation_split=0.1,
            epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0,
            callbacks=make_callbacks("bi")
        )
        y_pred_bi_sc = model_bi.predict(Xte, batch_size=BATCH_SIZE, verbose=0).squeeze(-1)

        # í‰ê°€(ìŠ¤ì¼€ì¼ ë‹¨ìœ„)
        y_true_sc = yte.squeeze(-1)
        res_uni = FoldResult(
            "LSTM-Uni", fi, len(Xtr), len(Xte),
            rmse=rmse(y_true_sc, y_pred_uni_sc),
            mae=mean_absolute_error(y_true_sc, y_pred_uni_sc),
            mape=mape(y_true_sc, y_pred_uni_sc)
        )
        res_bi = FoldResult(
            "LSTM-Bi", fi, len(Xtr), len(Xte),
            rmse=rmse(y_true_sc, y_pred_bi_sc),
            mae=mean_absolute_error(y_true_sc, y_pred_bi_sc),
            mape=mape(y_true_sc, y_pred_bi_sc)
        )
        all_results.extend([res_uni, res_bi])

        print(f"[Fold {fi}] Uni  RMSE={res_uni.rmse:.4f} MAE={res_uni.mae:.4f} MAPE={res_uni.mape:.2f}%")
        print(f"[Fold {fi}] Bi   RMSE={res_bi.rmse:.4f} MAE={res_bi.mae:.4f} MAPE={res_bi.mape:.2f}%")

        # === ë„ì‹í™”ë¥¼ ìœ„í•œ ì˜ˆì¸¡ ìˆ˜ì§‘ ===
        for (loc, d), yt, pu, pb in zip(meta_te, y_true_sc, y_pred_uni_sc, y_pred_bi_sc):
            if (loc_focus is None) or (loc == loc_focus):
                pred_rows.append((pd.to_datetime(d), float(yt), float(pu), float(pb)))

    # ìš”ì•½ í…Œì´ë¸”
    if not all_results:
        print("ìœ íš¨í•œ í´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        if return_predictions:
            return pd.DataFrame(columns=["date","y_true_sc","y_pred_uni_sc","y_pred_bi_sc"])
        return

    summary = (
        pd.DataFrame([r.__dict__ for r in all_results])
          .groupby("model_type")[["rmse","mae","mape"]]
          .mean()
          .sort_values("rmse")
    )
    print("\n=== êµì°¨ê²€ì¦ í‰ê·  ì„±ëŠ¥(ìŠ¤ì¼€ì¼ ë‹¨ìœ„) ===")
    print(summary)

    best = summary.index[0]
    print(f"\nğŸ‘‰ í‰ê·  RMSE ê¸°ì¤€ ìµœê³  ëª¨ë¸: {best}")

    # ìµœì¢… ì˜ˆì¸¡ DF ìƒì„± & ë¦¬í„´
    if return_predictions:
        df_pred = pd.DataFrame(
            pred_rows,
            columns=["date","y_true_sc","y_pred_uni_sc","y_pred_bi_sc"]
        ).sort_values("date").reset_index(drop=True)
        return df_pred

if __name__ == "__main__":
    # 1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë°ëª¨ ìƒì„±)
    if data_prex is not None:
        try:
            df = data_prex()  # columns: date, location_code, value
        except Exception:
            df = None
    else:
        df = None

    if df is None:
        # ë°ëª¨ìš© ë°ì´í„°
        dates = pd.date_range("2015-01-01","2025-08-01", freq="MS")
        rows = []
        for loc in ["11110","1174070000"]:
            base = 200000 if loc=="11110" else 20000
            trend = np.linspace(0, -15000, len(dates))
            season = 2000*np.sin(2*np.pi*np.arange(len(dates))/12)
            noise  = np.random.normal(0, 500, len(dates))
            # ì½”ë¡œë‚˜ êµ¬ê°„: 2020-04 ~ 2021-03 ê¸‰ë³€ (shock)
            shock = np.zeros(len(dates))
            mask = (dates>=pd.Timestamp("2020-04-01")) & (dates<=pd.Timestamp("2021-03-01"))
            shock[mask] = np.linspace(-6000, 4000, mask.sum())
            vals = base + trend + season + noise + shock
            for d,v in zip(dates, vals):
                rows.append((d, loc, float(max(v, 0))))
        df = pd.DataFrame(rows, columns=["date","location_code","value"])

    # 2) êµì°¨ê²€ì¦ + ì˜ˆì¸¡ ìˆ˜ì§‘ (íŠ¹ì • ì§€ì—­ë§Œ ë³´ê³  ì‹¶ìœ¼ë©´ loc_focus="11110")
    df_pred = run_cv_compare(df, loc_focus=None, return_predictions=True)

    # 3) ë„ì‹í™”
    if df_pred is not None and len(df_pred):
        plot_trends(df_pred)
    else:
        print("ë„ì‹í™”í•  ì˜ˆì¸¡ì´ ì—†ìŠµë‹ˆë‹¤. SEQ_LEN/N_SPLITS/ë°ì´í„° ê¸¸ì´ë¥¼ ì¡°ì •í•˜ì„¸ìš”.")